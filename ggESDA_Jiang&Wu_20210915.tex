\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------
%% my packages
\usepackage{graphicx} % table rotate
\usepackage{array} %table height
\usepackage{pdfpages} %include pdf
\usepackage{amsmath} %math equation
\usepackage{amssymb} %math (real number symbol)

%\usepackage{svg}

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
\usepackage{framed}
\usepackage{float} %for figure [H]
%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\newcommand{\imgdir}{../../../paperImage/}
%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
%\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Bo-Syue Jiang\\National Taipei University
\And Han-Ming Wu\\National Chengchi University}
\Plainauthor{Bo-Syue Jiang,Han-Ming Wu}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{ggESDA: An \proglang{R} Package for Exploratory Symbolic Data Analysis using \pkg{ggplot2}}
\Plaintitle{ggESDA: An R Package for Exploratory Symbolic Data Analysis using ggplot2}
\Shorttitle{ggESDA package in R}

%% - \Abstract{} almost as usual
\Abstract{
  This paper presents the \pkg{ggESDA} package, which we developed for exploratory symbolic data analysis in \proglang{R}. Based on \pkg{ggplot2} \cite{Wickham:2009}, the \pkg{ggESDA} package which is familiar programming structure with its parent provides a wide variety of graphical techniques such as histogram, 3D-scatterplot and radar plot. In addition, a general and  customized transformation function \code{classic2sym()} is implemented for generating a symbolic data table from classical data frame by clustering algorithm, \pkg{RSDA} \cite{Rojas:2015} function and user-defined method. wait for edit......
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{data visualization, symbolic data analysis, exploratory data analysis, \pkg{ggplot2} extensions, interval-valued data, \proglang{R}}
\Plainkeywords{data visualization, symbolic data analysis, exploratory data analysis, ggplot2 extensions, interval-valued data, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).




\begin{document}
%\SweaveOpts{concordance=TRUE} 
\section{Introduction}
"In Data Science the aim is to extract new knowledge from Standard, Big, and complex data. Often these data are unstructured with variables defined on different kinds of units. They can also be multi-sources (as mixtures of numerical and textual data, with images and networks)." \cite{Diday:2018}. The statement indicates that not only conventional data but the unstructured data, also known as symbolic data, is vital for data science. Rather than the classical data represented by a single value, symbolic data with measurements on $p$ random variables can be $p$-dimensional statistical units such as hypercubes or histograms. The field of symbolic data analysis (SDA) \cite{Billard+Diday:2007} is to broaden the application aspects of statistical methodologies, extend traditional cognition of a form of data unit and build a brand-new analysis system of data science. Recent developments in the field of big data analytics have led to a renewed interest in complex structure data such as symbolic data. As shown in Figure \ref{fig:trend}, the number of researches in SDA represents an increasing trend from 1998 to 2020, which outstands the importance of it during the years.



\begin{figure}[h]	
  		\centering	 			 	 
 	 		%\includegraphics[width=1\textwidth]{\imgdir Trend_SDA_1998_to_2020.png} 
\includegraphics{ggESDA_Jiang&Wu_20210915-TrendFig}
  		\caption{The number of "symbolic data analysis" or "interval-valued data" related articles in researches and applications according to PubMed and ScienceDirect online database over time from 1998 to 2020.}   		
  		\label{fig:trend}   			 		 
\end{figure}

Among ScienceDirect, Engineering and Computer Science lead the subject areas obviously, shown in Figure \ref{fig:subjectAreas}.

\begin{figure}[h]	
  		\centering	 			 	 
 	 		%\includegraphics[width=1\textwidth]{\imgdir subjectAreas_scienceDirect.png} 
\includegraphics{ggESDA_Jiang&Wu_20210915-subjectFig}
  		\caption{Top 10 researches and applications domains for SDA or interval-valued data (ScienceDirect) from 1998 to 2020} 
  		\label{fig:subjectAreas}   			 		 
\end{figure}


In practice, the symbolic data is often generated by aggregating massive datasets into intervals in order to make the management easy and appropriate. An interval-valued symbolic random variable $X$, taking values in interval, can be denoted such as $X = [a,b] \subset  R^{1}$, where $a \leq b$, and $a, b \in R^{1}$. Let the random variable $X$, for instance, be the weight, then $X = [50,100]$ represents the interval covering the weight of people. With the advent of big data analytic, interval-valued data is becoming more common and accessible than ever. The researches for interval-valued data such as the sign test for COVID-19 data \cite{sherwani:2021}, the prediction via regularized artificial neural network \cite{yang:2019}, a bivariate Bayesian method for regression models \cite{xu:2021}, etc.

Exploratory Data Analysis (EDA) \cite{Tukey:1977} is primarily used to see what data can reveal beyond the formal modeling or hypothesis testing task, provides an overview of raw datasets and obtains a general understanding about the variables and their relationships.




\section[Basic usage of ggESDA]{Basic usage of \pkg{ggESDA}}

\pkg{ggESDA} is now available from the Github at \url{https://github.com/kiangkiangkiang/ggESDA}. All reference manual documented by exported functions and introduction vignettes can also be download here. In the following section, we are going to illustrate the functionalities and syntaxes about \pkg{ggESDA}.


\section{why SDA plot (weakness of classical plot)}

\subsection{sol overstike}
For the conventional exploratory data analysis, it is always a severe challenge to deal with enormous datasets because conventional displays suffered from overstrikes of data points representing the value (scatterplot type displays) or overstrikes of line segments connecting values of neighboring variables. As a consequence, exploratory symbolic data analysis (SDA) becomes a preliminary yet essential tool for summarizing the main characteristics of a data set before appropriate statistical modeling can be applied. Besides escaping the problem mentioned above, SDA can effectively reduce observations in data, which will make the study focus on what we interesting instead of unnecessary information such as Figure~\ref{fig:compare}.

\begin{figure}[h]
\centering
\includegraphics{ggESDA_Jiang&Wu_20210915-compare}
\caption{\label{fig:compare} Compare classical data and symbolic data}
\end{figure}

In Figure~\ref{fig:compare}, we can clearly visualize the scatter plot in the right hands, which is represented by symbolic data and aggregated by K-means \cite{macqueen:1967}. 


\subsection{full information}

In the past, we would like to use barplot to visualize the frequency of categorical data, but that was merely represented the distribution of full data in that category. It cannot lead researchers to explore more details in what they are interesting such as a particular part of data, so aggregation methods play a vital role to merge the data we interesting.

However, the conventional categorical data after merging will usually be represented by mode, which will be unmeaningful to visualize and cause the loss of information that may become larger when the data or the number of factors in that category is growing on. SDA will build a histogram by calculating each factor of the category of frequency as bins to solve this kind of problem as a result. In that way, a categorical variable will never be shown as a single value at all, instead, a complete information histogram will be substituted.


\section{classical data to symbolic data}

\subsection{datasets}

We will apply the breast mass dataset, which is computed from a digitized image of a fine needle aspirate (FNA), to demonstrate how does a classical dataset transforms into a symbolic dataset. The breast mass dataset describe characteristics of the cell nuclei present in the image. It can be downloaded from the kaggle at \url{https://www.kaggle.com/uciml/breast-cancer-wisconsin-data?select=data.csv}. There are 569 observations and 32 variables in the dataset. We are going to store this dataset in \code{breastData} as data frame type in \proglang{R}, and the variables will be shown as follows:
\begin{Schunk}
\begin{Sinput}
> colnames(breastData)
\end{Sinput}
\begin{Soutput}
 [1] "id"                      "diagnosis"              
 [3] "radius_mean"             "texture_mean"           
 [5] "perimeter_mean"          "area_mean"              
 [7] "smoothness_mean"         "compactness_mean"       
 [9] "concavity_mean"          "concave points_mean"    
[11] "symmetry_mean"           "fractal_dimension_mean" 
[13] "radius_se"               "texture_se"             
[15] "perimeter_se"            "area_se"                
[17] "smoothness_se"           "compactness_se"         
[19] "concavity_se"            "concave points_se"      
[21] "symmetry_se"             "fractal_dimension_se"   
[23] "radius_worst"            "texture_worst"          
[25] "perimeter_worst"         "area_worst"             
[27] "smoothness_worst"        "compactness_worst"      
[29] "concavity_worst"         "concave points_worst"   
[31] "symmetry_worst"          "fractal_dimension_worst"
\end{Soutput}
\end{Schunk}

Except for the first two variables, they are all composed of mean, standard error, and "worst" in their own field respectively.

\subsection{K-means}

K-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. In \pkg{ggESDA}, the algorithm will be based on the \pkg{stats} package, and the number of k is a parameter that user can define themself:

\begin{Schunk}
\begin{Sinput}
> breastData <- dplyr::select(breastData, -id)
> breastData.sym <- classic2sym(breastData, groupby = "kmeans", k = 5)
> breastData.sym.i <- breastData.sym$intervalData
> as.data.frame(head(breastData.sym.i[, 1:4], 5))
\end{Sinput}
\begin{Soutput}
      diagnosis     radius_mean    texture_mean    perimeter_mean
1 B:0.04 M:0.96 [13.81 : 19.59] [11.89 : 39.28]  [91.56 : 132.40]
2 B:0.00 M:1.00 [15.50 : 24.25] [10.38 : 32.47] [102.90 : 166.20]
3 B:0.00 M:1.00 [20.73 : 28.11] [17.25 : 31.12] [135.70 : 188.50]
4 B:0.68 M:0.32 [11.84 : 16.30] [10.89 : 30.72]  [77.93 : 109.80]
5 B:0.98 M:0.02  [6.98 : 13.05]  [9.71 : 33.81]   [43.79 : 85.09]
\end{Soutput}
\end{Schunk}

The \code{id} is unused in this case, so we remove it by \pkg{dplyr}. Then using \code{classic2sym()} to aggregate \code{breastData}. It will return several result sets include clustering result and interval-valued data, etc. The interval-valued data can be extracted by \code{$intervalData}, and it will be presented by the package of \pkg{RSDA} type.

The \code{groupby} is a parameter that determine what kind of aggregation methods will be used. Whenever the K-means method is applied, the consequent \code{k} will become meaningful, whereas the other situation is not. It is also a default method when users have no input arguments in \code{groupby}.

\subsection{Hierarchical}

The second well-known clustering algorithm is called Hierarchical clustering \cite{Cecil:1966}, also called hierarchical cluster analysis or HCA. It can be performed with a distance matrix
calculated by raw data and used to present the distance of each cluster. In basic \proglang{R} package, it is also realized by \pkg{stats}, which the \pkg{ggESDA} is based on for implementing HCA:

\begin{Schunk}
\begin{Sinput}
> breastData.sym <- classic2sym(breastData, groupby = "hclust")
> breastData.sym.i <- breastData.sym$intervalData
\end{Sinput}
\end{Schunk}

Remark that the \code{k} parameter is not meaningful in the case without K-means clustering. In \code{classic2sym()}, the keywords of HCA is called \code{hclust}.

\subsection{particular variable}

Using a particular variable to merge different data is a common way for data analysis, too. \pkg{ggESDA} provides such as this concept in \code{classic2sym()} to analyze different factors of category variables, and merge the same factor into the symbolic data type:

\begin{Schunk}
\begin{Sinput}
> breastData.sym <- classic2sym(breastData, groupby = "diagnosis")
> breastData.sym.i <- breastData.sym$intervalData
> head(breastData.sym.i[, 1:4], 5)
\end{Sinput}
\begin{Soutput}
      radius_mean    texture_mean   perimeter_mean           area_mean
B  [6.98 : 17.85]  [9.71 : 33.81] [43.79 : 114.60]   [143.50 : 992.10]
M [10.95 : 28.11] [10.38 : 39.28] [71.90 : 188.50] [361.60 : 2,501.00]
\end{Soutput}
\end{Schunk}

In \code{breastData}, the only category variable is \code{diagnosis}, which means the diagnosis of breast tissues (M = malignant, B = benign). We put it as an input argument in \code{groupby} for merging different diagnosis results, and the interval-valued data of result sets will display its factor levels in row names.


\subsection{user defined}\label{sec:userDef}

In general, users may not always use the aggregation methods we provide, thus, besides generating a particular variable for the group, \pkg{ggESDA} facilitates the process through the min data and max data that user-defined.

For the demonstration, we will build both min data and max data using \code{runif}. Generate a uniform random variable to make sure that all min data are smaller than max data:

\begin{Schunk}
\begin{Sinput}
> minData <- runif(100, -100, -50)
> maxData <- runif(100, 50, 100)
> demoData <- data.frame(min = minData, max = maxData)
> demoData.sym <- classic2sym(demoData, groupby = "customize", 
+                             minData = demoData$min,
+                             maxData = demoData$max)
> demoData.sym.i <- demoData.sym$intervalData
> as.data.frame(head(demoData.sym.i, 5))
\end{Sinput}
\begin{Soutput}
                V1
1 [-75.85 : 63.98]
2 [-93.71 : 85.33]
3 [-64.99 : 94.69]
4 [-57.34 : 66.03]
5 [-66.02 : 50.95]
\end{Soutput}
\end{Schunk}

Then choose the \code{customize} argument in \code{groupby}, input which data are \code{minData} or \code{maxData}, and the transformation will be simply completed.

In order to simplify the process and make the preprocessing friendly, we develop these methods and let the people who want to analyze symbolic data easier. Overall, the conversion and essential concepts can be summarized in table \ref{tab:classic2sym}. 

% begin table classic2sym
\begin{table}[htbp]
  \centering
  \caption{Summary for \code{classic2sym()}}
  \setlength{\extrarowheight}{6pt}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
    \hline
    \multicolumn{6}{c}{\textbf{\code{classic2sym()}}} \\\hline
    \midrule
    \code{groupby} Args. & Transformation & Data Type & Moment & Cluster Result & Require Other Args.  \\\hline
    \midrule
    \code{kmeans} & K-means & Numeric/Category & Smaller data & V     & TRUE \\
    \code{hclust} & Hierarchical & Numeric/Category & Smaller data & V     & FALSE \\
    variable name & Variables aggregation & Numeric/Category & Own the pre-clustering group &       & FALSE \\
    \code{customize} & User-defined & Numeric & Connect with other packages &       & TRUE \\\hline
    \bottomrule
    \end{tabular}}%
  \label{tab:classic2sym}%
\end{table}%
% end table classic2sym




\section{Generalization and Extension}

As far as generalization and extension are concerned, the package provides a simple way for making connections with other useful packages, so that the result of common statistical or machine learning methods on other packages may be visualized as well using \pgk{ggESDA} if it is interval-valued. The following will discuss these explicitly. 

\subsection{General principle}\label{sec:genPrin}

Generally, it is not merely the well-known packages in \proglang{R} that can make a plot using it. As long as keeping some principle, it will be easily performed:

\begin{enumerate}
  \item Understand the data structure clearly if it is an object from other packages.
  \item Extract the min data and max data from it or make some necessary transformation.
  \item Classify the data you extract belong.
  \item Reorganized it and use \code{classic2sym()} for the final transformation.
  \item Visualize the front result using \pgk{ggESDA}.
\end{enumerate}

Because of the interval-valued data, all SDA packages studying in the same field will store and deal with the min and max data. Hence, the transformation method in section \ref{sec:userDef} plays a important role.
With the connection being built, it can be compatible with all other tools in \proglang{R}.


\subsection{Example for generalization}

For the demonstration, we consider two famous \proglang{R} packages for SDA, \pkg{HistDAWass} \cite{irpino2015} and \pkg{MAINT.Data} \cite{Silva2011}. Both of these make lots of contributions to the statistics of SDA, so we tend to make some analysis using these and plot the result with \pkg{ggESDA}.

\subsubsection[HistDAWass]{\pkg{HistDAWass}}

We use the principle in section \ref{sec:genPrin} to process \code{BLOOD} data in \pkg{HistDAWass}, first. With the method \pkg{HistDAWass} provided, it will be more convenient to get min and max data:

\begin{Schunk}
\begin{Sinput}
> library(HistDAWass)
> # Get min and max data
> blood.min <- get.MatH.stats(BLOOD, stat = "min")
> blood.max <- get.MatH.stats(BLOOD, stat = "max")
> blood <- data.frame(blood.min, blood.max)
> # Reorganized and Build ggESDA obj.
> blood.sym <- classic2sym(blood, groupby = "customize",
+                      minData = blood[, 2:4],
+                      maxData = blood[, 6:8])
> # Make names
> blood.names <- get.MatH.main.info(BLOOD)$varnames
> blood.i <- blood.sym$intervalData
> colnames(blood.i) <- blood.names
> head(as.data.frame(blood.i), 5)
\end{Sinput}
\begin{Soutput}
        Cholesterol      Hemoglobin      Hematocrit
1  [80.00 : 240.00] [12.00 : 15.00] [35.00 : 47.00]
2  [80.00 : 240.00] [10.50 : 14.00] [31.00 : 44.00]
3  [95.00 : 245.00] [10.50 : 14.00] [31.00 : 43.50]
4 [105.00 : 260.00] [10.50 : 14.00] [31.00 : 42.50]
5 [115.00 : 260.00] [10.80 : 13.60] [31.00 : 42.50]
\end{Soutput}
\end{Schunk}

After getting the necessary data, classifying data belonging is vital for reorganization, which means that differentiating the min data and max data. For instance, \code{minData = blood[, 2:4]} represents the min data are the columns of $2,3,4$ in this case.

\subsubsection[MAINT.Data]{\pkg{MAINT.Data}}

However, it is also a common way to store interval-valued data by median and range. In \pkg{MAINT.Data}, the data will exist in this form. Fortunately, a median-range form is not difficult to deal with. We can do the necessary conversion directly to get the data we expect:

\begin{Schunk}
\begin{Sinput}
> library(MAINT.Data)
> #get data interval-valued data in AbaloneIdt
> Aba.range <- AbaloneIdt@LogR
> Aba.mid <- AbaloneIdt@MidP
> #make a necessary transformation for build min max data
> Aba <- data.frame(Aba.min = Aba.mid - exp(Aba.range) / 2,
+                   Aba.max = Aba.mid + exp(Aba.range) / 2)
> # Reorganized and Build ggESDA obj.
> Aba.sym<- classic2sym(Aba, groupby = "customize",
+                       minData = Aba[, 1:7],
+                       maxData = Aba[, 8:14])
> # Make names
> colnames(Aba.sym$intervalData) <- AbaloneIdt@VarNames
> Aba.i <- Aba.sym$intervalData %>% 
+   cbind(Aba.obs = AbaloneIdt@ObsNames) %>% 
+   column_to_rownames(var = "Aba.obs")
> head(Aba.i[, 1:4], 5)
\end{Sinput}
\begin{Soutput}
               Length      Diameter        Height  Whole_weight
F-10-12 [0.34 : 0.78] [0.26 : 0.63] [0.06 : 0.23] [0.21 : 2.66]
F-13-15 [0.39 : 0.82] [0.30 : 0.65] [0.10 : 0.25] [0.27 : 2.51]
F-16-18 [0.40 : 0.74] [0.32 : 0.60] [0.10 : 0.24] [0.35 : 2.20]
F-19-21 [0.49 : 0.72] [0.36 : 0.58] [0.12 : 0.21] [0.68 : 2.12]
F-23-24 [0.45 : 0.80] [0.38 : 0.63] [0.14 : 0.22] [0.64 : 2.53]
\end{Soutput}
\end{Schunk}

In brief, following the general principle in section \ref{sec:genPrin} may facilitate the integration, extend utilize of \pkg{ggESDA} and generalize to all SDA studies.

\section{Prominent SDA Packages}

The most prominent packages on CRAN are commonly used for statistical or machine learning analyze. It can be briefly classified into two parts, one is focused on statistic analysis, and the other is general SDA packages including both analysis method and some graphical technology. Nevertheless, most of their graphical technology tends to use the basic graphics in \proglang{R} rather than \pkg{ggplot2}, or only visualizes univariate distribution which is difficult to present the relationship between variables. 

On the contrary, \pkg{ggESDA} uses a high-level graphic system by \pkg{ggplot2} to solve the problem mentioned above and provides a variety of EDA methods in all kinds of the variate, which can be summarized as the table \ref{tab:pkgCompare}. The number in table \ref{tab:pkgCompare} shows how many methods are provided in its field.

%begin : this is my table for package_compare 
%\begin{table}[htbp]
\begin{table}[h]
  \centering
  \caption{Compare with \proglang{R} packages}
  %\rotatebox[origin=c]{90}{
  \setlength{\extrarowheight}{6pt}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{|l|l|c|c|ccc|c|c|c|}
    \hline
    \toprule
    \multicolumn{1}{r}{} & \multicolumn{1}{r}{} & \multicolumn{1}{c}{\textbf{ Available}} & \multicolumn{1}{c}{\textbf{Summarize}} & \multicolumn{3}{c}{\textbf{EDA}} & \multicolumn{1}{c}{\textbf{Statistic}} & \multicolumn{2}{c}{\textbf{Machine learning}} \\\hline
    \multicolumn{1}{c|}{Package} & \multicolumn{1}{c|}{Author} & Version & Function & Univariate & Bivariate & Multivariate & Stat. Method & Supervised & \multicolumn{1}{c}{Unsupervised} \\\hline
    RSDA  & Rojas et al. (2015) & R(4.1.0)     & \code{classic.to.sym()}     & -     & 1     & -     & 3     & \textbf{2}    & \textbf{1} \\
    symbolicDA & Dudek et al. (2013) & R(4.1.0)     &    -   & -     & -     & \textbf{2}    & 2     & 1     & \textbf{1} \\
    HistDAWass & Irpino (2015) & R(4.1.0)     & \code{data2hist()}     & 4     & -     & -     & 3     & 1     & \textbf{1} \\
    \midrule
    MAINT.Data & Silva \&  Brito (2011) & R(4.1.0)     &   -    & -     & -     & -     & \textbf{7}    & -     & \textbf{1} \\\hline
    iRegression & Neto et al. (2011) & R(4.1.0)     &   -    & -     & -     & -     & 1     & -     & - \\
    intReg & Toomet (2012) & R(3.6.0)      &  -     & -     & -     & -     & 1     & -     & - \\
    ISDA.R & Filho \&  Fagundes (2012) & R(2.15.2)      &    -   & 1     & -     & 1     & 1     & -     & - \\
    GPCSIV & Brahim (2013) & R(3.0.2)      &  -     & -    & -     & -     & 1     & -     & - \\
    GraphPCA & Brahim \& Kallyth (2014) & R(4.1.0)      &   -    & -     & 1     & 1     & 1     & -     & - \\\hline
    \midrule
    ggESDA & Jiang (2021) & R(4.1.0)     & \code{classic2sym()}     & \textbf{8}    & \textbf{4}    & \textbf{2}    & 1     & -     & - \\\hline
    \bottomrule
    \end{tabular}}%
  \label{tab:pkgCompare}%
\end{table}%
%end : this is my table for package_compare 

In \proglang{python}, we can also find the SDA package such as \pkg{iardacil} \cite{umbleja:2020} which is available from the Github at \url{https://github.com/iardacil/SDA}. The zoomstart software in \pkg{iardacil} is provided with SODAS software project \cite{diday:2008}. It is a basic thinking for general radar plot, improved for distinct groups visualization by \pkg{ggESDA}, and implemented in \proglang{R} using \pkg{ggplot2}.


 
\section{General design}

The \pkg{ggESDA} object is composed of the interval-valued data, statistics data frame, clustering results, and other components from \pkg{R6} class. Based on it, the developed graphical technology can extend three aspects by its variables, univariate, bivariate, and multivariate. The \pkg{ggESDA} aims to convert the traditional data into the \pkg{ggESDA} object and visualizes the symbolic data using \pkg{ggplot2}, which is shown in Figure \ref{fig:pkgStr}.

\begin{figure}[h]	
  		\centering	 			 	
 	 		\includegraphics[width=1\textwidth]{doc/packageStructure2.eps} 
  		\caption{Package Structure and Diagram for the Transformation Flow} 
  		\label{fig:pkgStr}   			 		 
\end{figure}

As illustrated in Figure \ref{fig:pkgStr}, each row (observation) in the conventional data matrix $X_{p \times p}$ contains a vector of numeric values, $O_i = (x_1,x_2,\cdots,x_n)$, while each row of the interval-valued data matrix $I_{k \times p}$ contains a vector of intervals(ranges), $C_j = ([a_{j1},b_{j1}],[a_{j2},b_{j2}],\cdots,[a_{jp},b_{jp}])$, called a CONCEPT (or UNIT). The CONCEPT describe the behavior of a group of observations. Thus, the aggregation method between them is an essential process in SDA.



\section{Basic numerical summaries}

The main content of EDA relates to the basic numerical summaries of data (e.g., the central tendency measures, and variation or variability measures) and the basic graphical summaries of data. For example, the five-number summary of numerical data (minimum, $25\%$ quartiles, median, $70\%$ quartiles, and maximum) is used to construct a boxplot. In the field of SDA, there are many algorithms to calculate descriptive statistics and frequency for interval-valued data, and we will illustrate the univariate and bivariate summaries respectively.

\subsection{Univariate summaries}

To build a statistic chart or analysis, descriptive statistics are necessary to be constructed, as well as the frequency occurring in each bin in a histogram chart. For a histogram chart, subdivisions of it into equidistant and non-equidistant will also be consider in this section.

\subsubsection{Descriptive statistics}

For the quantile in interval-valued data, summarizing it may seem to be obvious to separate data into a minimum and maximum data table, then calculate quantiles of both data tables to build a new interval-valued quantile data table.

In statistics, it may be more interesting to discuss mean and variance in a particular random variable $Z$; see \cite{bertrand:2000}. The realization of $Z$ for the observation $W_u$ is the interval $Z(W_u) = [a_u,b_u]$, where $u=1,2,\cdots,m$ and $m$ is the number of concepts.

First of all, assume that each object is equally likely to be observed with probability $\frac{1}{m}$, and the empirical density function of $Z$ is defined as : 

\begin{equation}\label{eq:dist1}
f(\xi) = \frac{1}{m} \sum_{u:\xi \in Z(W_u)}(\frac{1}{b_u-a_u})
\end{equation}

where $\xi$ is the individual descriptions.


The Equation (\ref{eq:dist1}) is also equivalently to :

\begin{equation}\label{eq:dist2}
f(\xi) = \frac{1}{m}\sum_{u \in E}\frac{I_u(\xi)}{\| Z(u) \|},\xi \in \mathbb{R}
\end{equation}

where $I_u(.)$ is the indicator function that $\xi$ is or is not in the interval $Z(u)$, $\| Z(u) \|$ is the length of that interval, and $E=\{w_1,w_2,\cdots,w_m \}$.

Further, the symbolic sample mean from definition for $Z$ is $\bar{Z} = \int_{-\infty}^{\infty} \xi f(\xi)\;d\xi$, which can be reduced as :

\begin{equation}\label{eq:mean}
\bar{Z}=\frac{1}{m}\sum_{u \in E}\frac{a_u+b_u}{2}
\end{equation}

Finally, after getting the sample mean, the symbolic sample variance can be defined as follow:

\begin{equation}\label{eq:varDef}
\begin{split}
S^2 & = \int_{-\infty}^{\infty} (\xi - \bar{z})^2f(\xi)\;d\xi \\
 & = \int_{-\infty}^{\infty} \xi^2f(\xi)\;d\xi-\bar{z}
\end{split}
\end{equation}

and substituting for $f(\xi)$ from Equation (\ref{eq:dist2}), we have 
\begin{equation}\label{eq:derived}
\begin{split}
\int_{-\infty}^{\infty} \xi^2f(\xi)\;d\xi &= \frac{1}{m}\sum_{u \in E}\int_{-\infty}^{\infty} \xi^2 \frac{I_u(\xi)}{\| Z(u) \|}\;d\xi \\
&= \frac{1}{m}\sum_{u \in E} \int_{a_u}^{b_u} \frac{\xi^2}{(b_u-a_u)}\;d\xi \\
&= \frac{1}{3m} \sum_{u \in E}(\frac{b_u^3 - a_u^3}{b_u-a_u})
\end{split}
\end{equation}

Hence,
\begin{equation}\label{eq:var}
S^2 = \frac{1}{3m} \sum_{u \in E}(a_u^2+a_ub_u+b_u^2)-\frac{1}{4m^2}\left[ \sum_{u \in E}(a_u+b_u) \right]^2
\end{equation}

\subsubsection{Histogram Frequency}

For the univariate histogram frequency, assume that we partition the interval $I=[\min_{u \in E} a_u,\\ \max_{u \in E} b_u]$ into $r$ subintervals, and all of them in the histogram are equal distance. That is, $I_g = [\zeta_{g-1},\zeta_{g}),g=1,2,\cdots,r$, then $\| I_j \| = \| I_k \| , j,k=1,2,\cdots,r$. As a consequence, the observed frequency of the interval-valued variate $Z$ for the histogram subinterval $I_g$ from the definitions is

\begin{equation}\label{eq:fg}
f_g = \sum_{u \in E}\frac{\| Z(u) \cap I_g \|}{\| Z(u) \|}
\end{equation}

Moreover, for the interval-valued variate $Z$, we can pool the $a_u$ and $b_u$ from the interval of all observations, and sort it as a new vector $(x^{(1)},x^{(2)},\ctors,x^{(2m)})$ to represent the cut of a histogram. The subinterval from the cut is then defined as $I'_g = [x^{(j)},x^{(j+1)})$, where $j = 1,2,\cdots, 2m-1$, and apply the Equation (\ref{eq:fg}) to get frequency. In most cases, $\| I'_g \|$ will not be equal to another, so we can get another histogram type, called non-equidistant-bin histogram.


\subsection{Bivariate summaries}

\subsubsection{Descriptive statistics}

\subsubsection{2D-Histogram}





\bibliography{refs}

\end{document}
